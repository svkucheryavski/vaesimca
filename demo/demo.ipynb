{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model and making predictions\n",
    "\n",
    "In order to train and optimize VAESIMCA model, first we need to define encoder and decoder classes for the Variational Autoencoder. The classes must inherit `torch.nn.Module` class and the constructor must have two parameters: `img_size` — which is a tuple with image width, height and number of channels, and `latent_dim` — the desired dimension of the latent space.\n",
    "\n",
    "Here is an example of simple encoder and decoder similar to what have been used in the examples from the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all packages needed to run the code in this notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch import nn, sigmoid\n",
    "from torch.nn import functional as F\n",
    "from vaesimca import VAESIMCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        img_width, img_height, nchannels = img_size\n",
    "        self.conv1 = nn.Conv2d(nchannels, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(256 * (img_width // 16) * (img_height // 16), latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * (img_width // 16) * (img_height // 16), latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# define the decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_size, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        img_width, img_height, nchannels = img_size\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc = nn.Linear(latent_dim, 256 * (img_width // 16) * (img_height // 16))\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, nchannels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 256, (self.img_width // 16), (self.img_height // 16))\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = sigmoid(self.deconv4(z))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step is to define a sequence of transformations for the images. This sequence should at least include the resizing (to ensure that all images have the same size) and normalization to [0, 1] range which can be done by simply applying `toTensor()` transformation. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuple with image size (width, height, number of channels)\n",
    "img_size = (128, 128, 1)\n",
    "\n",
    "# define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([img_size[0], img_size[1]]),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we also set `img_size` value for the simulated images.\n",
    "\n",
    "After that you need to define path to the training set. The directory should contain an image folder whose name will be the same as a target class name you want to train the model for. If the directory also contains other folders, they will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory with training sets\n",
    "train_path = \"images_simulated/train\"\n",
    "\n",
    "# class name (directory with this name should be in the training path)\n",
    "classname = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the model (here in all examples the number of epochs is relatively small to make computation faster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "latent_dim = 6\n",
    "nepochs = 30\n",
    "lr = 0.0001\n",
    "\n",
    "# initialize the model object\n",
    "m = VAESIMCA(encoder_class=Encoder, decoder_class=Decoder, classname = classname,\n",
    "                img_size=img_size, latent_dim=latent_dim, transform=transform)\n",
    "\n",
    "# train the model\n",
    "m.fit(data_path=train_path, nepochs=nepochs, lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default the learning rate is getting smaller every 10 epochs. This can be changed by providing corresponding parameters, `scheduler_step_size` (by default is 10, meaning change learning rate after each 10 epochs) and `scheduler_gamma` (by default 0.5 meaning decrease the rate by half).\n",
    "\n",
    "The `train()` method has another argument, `verbose`, which is by default set to `True`. In this case the training process will provide some information every 10 epochs (the training lost). To suppress this simply set the parameter to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model without output\n",
    "m.fit(data_path=train_path, nepochs=nepochs, lr = lr, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `fit()` also has several other parameters which can influence the training process, including `beta` — regularisation factor for computing total loss, batch size, etc. Check help for the `fit()` method for more details.\n",
    "\n",
    "Once the model is trained one can start using it to make predictions for other images. Here is an example how to make predictions for the training set and for the test set which contains three classes (`target`, `alt1`, `alt2`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make predictions and show summary for training set\n",
    "rc = m.predict(data_path=train_path)\n",
    "rc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and show summary for test set located in a separate folder\n",
    "test_path = \"images_simulated/test\"\n",
    "rt = m.predict(data_path=test_path)\n",
    "rt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the predictions are made based on the full distance values. You can change this by providing additional parameter, `distance`, which can be set to `f` (full distance, default value), `q` (residual distance) and `h` (explained distance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = m.predict(data_path=test_path, distance=\"q\")\n",
    "rt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict()` method returns the `VAESIMCARes` object which contains all outcomes: distances, scores, etc. It has several methods which let you assess the performance of the model. For each class found in the directory `data_path`, it simply computes the number of objects which were accepted by the model as target class members (`in`) or rejected as strangers (`out`).\n",
    "\n",
    "Method `summary()` prints this information a readable way, but it can also be obtained as a dictionary by using method `stat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results of classification for each class as a dictionary\n",
    "s = rt.stat()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the prediction is made for significance level of 0.05, which means that 5% of target class members will be rejected (in theory), so the expected sensitivity of the model is 0.95. This can be changed by providing additional parameter to the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using decision boundary computed for SL = 0.01\n",
    "rt2 = m.predict(data_path=test_path, alpha = 0.01)\n",
    "rt2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can also be visualized using two plots. The first is the *Distance plot*, which shows the distance values in form of bar plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 10), dpi = 150)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "rt.plotDistance(plt, distance=\"h\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "rt.plotDistance(plt, distance=\"q\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "rt.plotDistance(plt, distance=\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each distance you can show the statistical boundaries (for detection of extreme objects and outliers) as well as the object labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi = 200)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "rt.plotDistance(plt, distance=\"h\", show_boundaries=True)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "rt.plotDistance(plt, distance=\"q\", show_boundaries=True)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "rt.plotDistance(plt, distance=\"f\", show_boundaries=True, show_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is the *Acceptance plot*, which shows the scaled explained and residual distances in form of a scatter plot together with the decision boundary. It is also possible to show the object labels and use log-transformed distance values, which are in general improve the plot readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi = 150)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "rt.plotAcceptance(plt)\n",
    "plt.subplot(1, 2, 2)\n",
    "rt.plotAcceptance(plt, dolog = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both plots, user can modify colors used to show outcomes of each class as well as the markers in case of the acceptance plot. Check the documentation of the methods for details.\n",
    "\n",
    "Finally one can get full set of results including object labels, distances, decision etc. as a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = rt.as_df()\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `decision` column is logical, `True` if object was accepted as a member of the target class by the model and `False` if it was rejected. Same about the `outlier` object. The sample labels are image file names without the extension.\n",
    "\n",
    "Finally, you can also plot the reconstruction error for an image with given label from a given class (you need to specify both to avoid possible ambiguity as images in different folders may have the same names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "rt.plotError(plt, classname = \"target\", object_label=\"t64\")\n",
    "plt.subplot(2, 2, 2)\n",
    "rt.plotError(plt, classname = \"target\", object_label=\"t14\")\n",
    "plt.subplot(2, 2, 3)\n",
    "rt.plotError(plt, classname = \"alt1\", object_label=\"a64\")\n",
    "plt.subplot(2, 2, 4)\n",
    "rt.plotError(plt, classname = \"alt2\", object_label=\"a64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of the model parameters\n",
    "\n",
    "The `VAESIMCA` class has a static method `gridsearch()` which simplifies optimization of four main parameters: the dimension of the latent space (`ld`), the learning rate (`lr`), the batch size (`bs`) and the regularization parameter (`beta`). It requires both training and test set for the optimization as well as other components and settings necessary to train the `VAESIMCA` model: encoder and decoder classes, transformation function, number of epochs, etc.\n",
    "\n",
    "By default it runs optimization for each combination of parameters three times (three iterations), but this can be changed.\n",
    "\n",
    "Here is an example, where it trains the models for different combination of the parameters and returns results in form of a data frame which contains the learning parameters and number and percent of objects in each class that was accepted by the corresponding model. During each iteration it shows progress bar as well as best sensitivity, specificity and efficiency achieved. At the end it shows parameters which gave the best sensitivity, specificity and efficiency overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch example — !!! note it will take time to run this code !!!\n",
    "\n",
    "train_path = \"images_simulated/train\"\n",
    "test_path = \"images_simulated/test\"\n",
    "classname = \"target\"\n",
    "img_size = (128, 128, 1)\n",
    "\n",
    "nepochs = 30\n",
    "lr_seq = [10E-4, 1E-5]\n",
    "ld_seq = [8, 16]\n",
    "bs_seq = [10, 20]\n",
    "beta_seq = [0.5, 1]\n",
    "\n",
    "res = VAESIMCA.gridsearch(\n",
    "    train_path=train_path, test_path=test_path, classname = classname, img_size=img_size, transform=transform,\n",
    "    encoder_class=Encoder, decoder_class=Decoder,\n",
    "    lr_seq=lr_seq, ld_seq=ld_seq, bs_seq=bs_seq, beta_seq=beta_seq,\n",
    "    nepochs=nepochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reuse the encoder/decoder classes and transformation function from the examples above. If you just need the results you can suppress the output by providning parameter `verbose = False`.\n",
    "\n",
    "Here how the returned results look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be then combined to get overall statistics for each combination of the parameters, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to compute sensitivity and specificity for test set only\n",
    "def compute_metrics(df):\n",
    "    metrics = []\n",
    "\n",
    "    # Filter the dataframe to include only the test set\n",
    "    test_df = df[df['set'] == 'test']\n",
    "\n",
    "    # Group by unique combinations of the parameters\n",
    "    grouped = test_df.groupby(['comb', 'beta', 'bs', 'lr', 'ld'])\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # Extract target class information\n",
    "        target_group = group[group['class'] == 'target']\n",
    "        alt_group = group[group['class'] != 'target']\n",
    "\n",
    "        # Calculate sensitivity\n",
    "        sensitivity = target_group['in'].sum() / target_group['n'].sum()\n",
    "\n",
    "        # Calculate specificity\n",
    "        specificity = (alt_group['n'].sum() - alt_group['in'].sum()) / alt_group['n'].sum()\n",
    "\n",
    "        # Append the results\n",
    "        metrics.append({\n",
    "            'comb': name[0],\n",
    "            'beta': name[1],\n",
    "            'bs': name[2],\n",
    "            'lr': name[3],\n",
    "            'ld': name[4],\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "stat = compute_metrics(res)\n",
    "print(stat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_specificity = stat.groupby('ld')['specificity'].apply(list)\n",
    "\n",
    "# Create the box plot using matplotlib\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(grouped_specificity, labels=grouped_specificity.index)\n",
    "plt.title('Specificity vs. LD')\n",
    "plt.xlabel('LD')\n",
    "plt.ylabel('Specificity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
